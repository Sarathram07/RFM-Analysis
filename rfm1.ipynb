{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark RFM example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark.read.format('com.databricks.spark.csv').\\\n",
    "                       options(header='true', \\\n",
    "                       inferschema='true').\\\n",
    "            load(\"data.csv\",header=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.show(5)\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "def my_count(df_in):\n",
    "    df_in.agg( *[ count(c).alias(c) for c in df_in.columns ] ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|   541909|   541909|     540455|  541909|     541909|   541909|    406829| 541909|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_count(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|   406829|   406829|     406829|  406829|     406829|   406829|    406829| 406829|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_raw.dropna(how='any')\n",
    "my_count(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_utc_timestamp, unix_timestamp, lit, datediff, col\n",
    "\n",
    "timeFmt = \"MM/dd/yy HH:mm\"\n",
    "\n",
    "df = df.withColumn('NewInvoiceDate'\n",
    "                 , to_utc_timestamp(unix_timestamp(col('InvoiceDate'),timeFmt).cast('timestamp')\n",
    "                 , 'UTC'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "df = df.withColumn('TotalPrice', round( df.Quantity * df.UnitPrice, 2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guhanesvar/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 1.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, min, max, sum, datediff, to_date\n",
    "\n",
    "date_max = df.select(max('NewInvoiceDate')).toPandas()\n",
    "current = to_utc_timestamp( unix_timestamp(lit(str(date_max.iloc[0][0])), \\\n",
    "                              'yy-MM-dd HH:mm').cast('timestamp'), 'UTC' )\n",
    "\n",
    "df = df.withColumn('Duration', datediff(lit(current), 'NewInvoiceDate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "recency = df.groupBy('CustomerID').agg(min('Duration').alias('Recency'))\n",
    "frequency = df.groupBy('CustomerID', 'InvoiceNo').count()\\\n",
    "                        .groupBy('CustomerID')\\\n",
    "                        .agg(count(\"*\").alias(\"Frequency\"))\n",
    "monetary = df.groupBy('CustomerID').agg(round(sum('TotalPrice'), 2).alias('Monetary'))\n",
    "rfm = recency.join(frequency,'CustomerID', how = 'inner')\\\n",
    "             .join(monetary,'CustomerID', how = 'inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+--------+\n",
      "|CustomerID|Recency|Frequency|Monetary|\n",
      "+----------+-------+---------+--------+\n",
      "|     15619|     10|        1|   336.4|\n",
      "|     17389|      0|       43|31300.08|\n",
      "|     12940|     46|        4|  876.29|\n",
      "|     13623|     30|        7|  672.44|\n",
      "|     14450|    180|        3|  483.25|\n",
      "+----------+-------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_pd(df_in, columns, deciles=False):\n",
    "\n",
    "    if deciles:\n",
    "        percentiles = np.array(range(0, 110, 10))\n",
    "    else:\n",
    "        percentiles = [25, 50, 75]\n",
    "\n",
    "    percs = np.transpose([np.percentile(df_in.select(x).collect(), percentiles) for x in columns])\n",
    "    percs = pd.DataFrame(percs, columns=columns)\n",
    "    percs['summary'] = [str(p) + '%' for p in percentiles]\n",
    "\n",
    "    spark_describe = df_in.describe().toPandas()\n",
    "    new_df = pd.concat([spark_describe, percs],ignore_index=True)\n",
    "    new_df = new_df.round(2)\n",
    "    return new_df[['summary'] + columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>Recency</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Monetary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>4372</td>\n",
       "      <td>4372</td>\n",
       "      <td>4372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>91.58119853613907</td>\n",
       "      <td>5.07548032936871</td>\n",
       "      <td>1898.4597003659658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>100.77213931384833</td>\n",
       "      <td>9.338754163574729</td>\n",
       "      <td>8219.345141139722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-4287.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>373</td>\n",
       "      <td>248</td>\n",
       "      <td>279489.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4287.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10%</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20%</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>234.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30%</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>337.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40%</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>465.412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50%</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>648.075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>60%</td>\n",
       "      <td>71.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>909.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>70%</td>\n",
       "      <td>108.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1311.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>80%</td>\n",
       "      <td>178.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2002.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>90%</td>\n",
       "      <td>263.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3505.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>100%</td>\n",
       "      <td>373.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>279489.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   summary             Recency          Frequency            Monetary\n",
       "0    count                4372               4372                4372\n",
       "1     mean   91.58119853613907   5.07548032936871  1898.4597003659658\n",
       "2   stddev  100.77213931384833  9.338754163574729   8219.345141139722\n",
       "3      min                   0                  1            -4287.63\n",
       "4      max                 373                248           279489.02\n",
       "5       0%                 0.0                1.0            -4287.63\n",
       "6      10%                 4.0                1.0             146.022\n",
       "7      20%                11.0                1.0             234.392\n",
       "8      30%                21.0                1.0              337.37\n",
       "9      40%                31.0                2.0             465.412\n",
       "10     50%                50.0                3.0             648.075\n",
       "11     60%                71.0                4.0             909.134\n",
       "12     70%               108.0                5.0            1311.979\n",
       "13     80%               178.0                7.0            2002.092\n",
       "14     90%               263.0               11.0              3505.6\n",
       "15    100%               373.0              248.0           279489.02"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['Recency','Frequency','Monetary']\n",
    "describe_pd(rfm,cols,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RScore(x):\n",
    "    if  x <= 16:\n",
    "        return 1\n",
    "    elif x<= 50:\n",
    "        return 2\n",
    "    elif x<= 143:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "def FScore(x):\n",
    "    if  x <= 1:\n",
    "        return 4\n",
    "    elif x <= 3:\n",
    "        return 3\n",
    "    elif x <= 5:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def MScore(x):\n",
    "    if  x <= 293:\n",
    "        return 4\n",
    "    elif x <= 648:\n",
    "        return 3\n",
    "    elif x <= 1611:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "\n",
    "R_udf = udf(lambda x: RScore(x), StringType())\n",
    "F_udf = udf(lambda x: FScore(x), StringType())\n",
    "M_udf = udf(lambda x: MScore(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+--------+-----+-----+-----+\n",
      "|CustomerID|Recency|Frequency|Monetary|r_seg|f_seg|m_seg|\n",
      "+----------+-------+---------+--------+-----+-----+-----+\n",
      "|     15619|     10|        1|   336.4|    1|    4|    3|\n",
      "|     17389|      0|       43|31300.08|    1|    1|    1|\n",
      "|     12940|     46|        4|  876.29|    2|    2|    2|\n",
      "|     13623|     30|        7|  672.44|    2|    1|    2|\n",
      "|     14450|    180|        3|  483.25|    4|    3|    3|\n",
      "+----------+-------+---------+--------+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfm_seg = rfm.withColumn(\"r_seg\", R_udf(\"Recency\"))\n",
    "rfm_seg = rfm_seg.withColumn(\"f_seg\", F_udf(\"Frequency\"))\n",
    "rfm_seg = rfm_seg.withColumn(\"m_seg\", M_udf(\"Monetary\"))\n",
    "rfm_seg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+--------+-----+-----+-----+--------+\n",
      "|CustomerID|Recency|Frequency|Monetary|r_seg|f_seg|m_seg|RFMScore|\n",
      "+----------+-------+---------+--------+-----+-----+-----+--------+\n",
      "|     12471|      2|       49|18740.92|    1|    1|    1|     111|\n",
      "|     18161|     10|        6| 1612.79|    1|    1|    1|     111|\n",
      "|     17809|     16|       15| 4627.62|    1|    1|    1|     111|\n",
      "|     17754|      0|        6| 1739.92|    1|    1|    1|     111|\n",
      "|     15382|     14|        8| 5927.86|    1|    1|    1|     111|\n",
      "+----------+-------+---------+--------+-----+-----+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfm_seg = rfm_seg.withColumn('RFMScore',\n",
    "                             F.concat(F.col('r_seg'),F.col('f_seg'), F.col('m_seg')))\n",
    "rfm_seg.sort(F.col('RFMScore')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+------------------+------------------+\n",
      "|RFMScore|     avg(Recency)|     avg(Monetary)|    avg(Frequency)|\n",
      "+--------+-----------------+------------------+------------------+\n",
      "|     111|6.035123966942149| 8828.888595041324|18.882231404958677|\n",
      "|     112|7.237113402061856|1223.3604123711339| 7.752577319587629|\n",
      "|     113|              8.0|          505.9775|               7.5|\n",
      "|     114|             11.0|            191.17|               8.0|\n",
      "|     121|6.472727272727273|2569.0619999999994| 4.636363636363637|\n",
      "+--------+-----------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfm_seg.groupBy('RFMScore')\\\n",
    "       .agg({'Recency':'mean',\n",
    "             'Frequency': 'mean',\n",
    "             'Monetary': 'mean'} )\\\n",
    "        .sort(F.col('RFMScore')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [r[0],Vectors.dense(r[1:])]).toDF(['CustomerID','rfm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|CustomerID|                rfm|\n",
      "+----------+-------------------+\n",
      "|     15619|   [10.0,1.0,336.4]|\n",
      "|     17389|[0.0,43.0,31300.08]|\n",
      "|     12940|  [46.0,4.0,876.29]|\n",
      "|     13623|  [30.0,7.0,672.44]|\n",
      "|     14450| [180.0,3.0,483.25]|\n",
      "+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed= transData(rfm)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------------------------------------------------------------+\n",
      "|CustomerID|rfm                |features                                                       |\n",
      "+----------+-------------------+---------------------------------------------------------------+\n",
      "|15619     |[10.0,1.0,336.4]   |[0.02680965147453083,0.0,0.016294610567853272]                 |\n",
      "|17389     |[0.0,43.0,31300.08]|[0.0,0.1700404858299595,0.12540746393334334]                   |\n",
      "|12940     |[46.0,4.0,876.29]  |[0.12332439678284182,0.012145748987854251,0.01819712791732512] |\n",
      "|13623     |[30.0,7.0,672.44]  |[0.08042895442359249,0.024291497975708502,0.017478781288030567]|\n",
      "|14450     |[180.0,3.0,483.25] |[0.48257372654155495,0.008097165991902834,0.016812095004997765]|\n",
      "+----------+-------------------+---------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"rfm\",\\\n",
    "         outputCol=\"features\")\n",
    "scalerModel =  scaler.fit(transformed)\n",
    "scaledData = scalerModel.transform(transformed)\n",
    "scaledData.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.8646368936683079\n",
      "Silhouette Score: 0.8045154385557955\n",
      "Silhouette Score: 0.7008699963021162\n",
      "Silhouette Score: 0.6716569820272631\n",
      "Silhouette Score: 0.6179340289777181\n",
      "Silhouette Score: 0.44971691376104345\n",
      "Silhouette Score: 0.6299608212562088\n",
      "Silhouette Score: 0.6590769573879091\n",
      "Silhouette Score: 0.6591924486884344\n",
      "Silhouette Score: 0.6457686374293314\n",
      "Silhouette Score: 0.6658159824942368\n",
      "Silhouette Score: 0.6410023270519241\n",
      "Silhouette Score: 0.6501056841336909\n",
      "Silhouette Score: 0.6315474100955457\n",
      "Silhouette Score: 0.6208600367405319\n",
      "Silhouette Score: 0.6412770695206362\n",
      "Silhouette Score: 0.6383045017805672\n",
      "Silhouette Score: 0.6197672019086612\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import col, percent_rank, lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.sql.types import StructType\n",
    "from functools import reduce  \n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "silhouette_score=[]\n",
    "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='features', \\\n",
    "                                metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "for i in range(2,20):\n",
    "    \n",
    "    KMeans_algo=KMeans(featuresCol='features', k=i)\n",
    "    \n",
    "    KMeans_fit=KMeans_algo.fit(scaledData)\n",
    "    \n",
    "    output=KMeans_fit.transform(scaledData)\n",
    "    \n",
    "    \n",
    "    \n",
    "    score=evaluator.evaluate(output)\n",
    "    \n",
    "    silhouette_score.append(score)\n",
    "    \n",
    "    print(\"Silhouette Score:\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0a632fbb8bf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaledData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaledData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KMeans' is not defined"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "kmeans = KMeans().setK(k).setSeed(1)\n",
    "model = kmeans.fit(scaledData)\n",
    "# Make predictions\n",
    "predictions = model.transform(scaledData)\n",
    "predictions.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+--------+----------+\n",
      "|CustomerID|Recency|Frequency|Monetary|prediction|\n",
      "+----------+-------+---------+--------+----------+\n",
      "|     13098|      1|       41|28658.88|         1|\n",
      "|     13248|    124|        2|  465.68|         4|\n",
      "|     13452|    259|        2|   590.0|         0|\n",
      "|     13460|     29|        2|  183.44|         1|\n",
      "|     13518|     85|        1|  659.44|         3|\n",
      "+----------+-------+---------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = rfm.join(predictions.select('CustomerID','prediction'),'CustomerID',how='left')\n",
    "results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+\n",
      "|prediction|      avg(Recency)|     avg(Monetary)|    avg(Frequency)|\n",
      "+----------+------------------+------------------+------------------+\n",
      "|         0|243.48979591836735|494.17213151927456|1.7052154195011338|\n",
      "|         1|16.183703703703703|3231.2944000000007| 8.018765432098766|\n",
      "|         2| 333.9367088607595| 324.9553797468355|1.5411392405063291|\n",
      "|         3|  68.6654028436019|1012.9810805687206| 3.177251184834123|\n",
      "|         4|153.78691588785045| 686.6986915887854|2.5439252336448597|\n",
      "+----------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.groupBy('prediction')\\\n",
    "       .agg({'Recency':'mean',\n",
    "             'Frequency': 'mean',\n",
    "             'Monetary': 'mean'} )\\\n",
    "        .sort(F.col('prediction')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
